{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2048057_Program_6.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sx0n1uOFiEOp"
      },
      "source": [
        "---\n",
        "# <center> **NLP PROGRAM-6** </center>\n",
        "## <center> A program for stemming Non-English words </center>\n",
        "#### <center> Soundarya G_ 2048057</center>\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkmCal8SxZ2y"
      },
      "source": [
        "* Use a minimum of 5 Stemmers(Specific to the language)  and maximum N \n",
        "* Compare each output with any one of the stemmers used before( English:Porter stemmer)\n",
        "* Interpret the inflection for each output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkzdEi0snp0n"
      },
      "source": [
        "# Downloading and Importing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJXWxxmkhvAz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41b8e543-e7bc-4e3d-d34a-608ef85e2df5"
      },
      "source": [
        "# Import nltk \n",
        "import nltk \n",
        "nltk.download('all')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRUcH4_lno6y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3c65774-cbfb-4034-9059-ed5637254a74"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzZgDQrC5sNQ",
        "outputId": "632abc30-5922-4264-aa0f-3a49814c3563"
      },
      "source": [
        "!pip install Tashaphyne"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Tashaphyne in /usr/local/lib/python3.7/dist-packages (0.3.4.1)\n",
            "Requirement already satisfied: pyarabic in /usr/local/lib/python3.7/dist-packages (from Tashaphyne) (0.6.14)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from pyarabic->Tashaphyne) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQWMZR_zlYQM"
      },
      "source": [
        "# Stemming\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_CDdn2S0Lwm"
      },
      "source": [
        "* Stemming is also a type of text normalization that enables you to standardize some words into specific expressions also called stems. \n",
        "\n",
        "* Stemming is basically removing the suffix from a word and reduce it to its root word. \n",
        ">For example: “Flying” is a word and its suffix is “ing”, if we remove “ing” from “Flying” then we will get base word or root word which is “Fly”\n",
        "\n",
        "* Stemming VS Lemmatization\n",
        "> Stemming: Produces a words \"stem\" which is produced by stemmers. \n",
        "\n",
        "         Ex: am -am\n",
        "             the goin -the go\n",
        "             having -hav\n",
        "> Lemmatization: Produces words called \"lemma\" which is produced by lemmatizers.\n",
        "\n",
        "         Ex: am -be\n",
        "             the going -the going\n",
        "             having -have\n",
        "\n",
        "* Application of Stemming and Lemmatization\n",
        "\n",
        "         - Sentimental Analysis\n",
        "         - Document Clustering\n",
        "         - Information Retrieval: search engines.\n",
        "         - To determine domain vocabularies in domain analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKsMeB7joEkV"
      },
      "source": [
        "# Stemmers for Non-English words\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYFk3UTJ0hYg"
      },
      "source": [
        "There are different types of stemmers that supports english and non-english words\n",
        "\n",
        "    - Porter Stemmer\n",
        "    - Lancaster Stemmer\n",
        "    - Lovins Stemmer \n",
        "    - Dawson Stemmer \n",
        "    - Krovetz Stemmer \n",
        "    - Snowball Stemmer\n",
        "    - ISRI Stemmer\n",
        "    - RSLPS Stemmer\n",
        "    - Regexp Stemmer\n",
        "    - Germen Stemmer\n",
        "    - Cistem Stemmer\n",
        "    - Lancaster Stemmer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sP1CpSf_sP8"
      },
      "source": [
        "Python nltk provides not only two English stemmers: PorterStemmer and LancasterStemmer but also a lot of non-English stemmers as part of SnowballStemmers, ISRIStemmer, RSLPSStemmer. Python NLTK included SnowballStemmers as a language to create to create non-English stemmers. One can program one's own language stemmer using snowball. Currently, it supports the following languages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxVm-imYlFe2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a18ca1e-2dfe-4768-bba0-b115b8e4fcb0"
      },
      "source": [
        "# See which languages are supported\n",
        "print(\" \".join(SnowballStemmer.languages)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "arabic danish dutch english finnish french german hungarian italian norwegian porter portuguese romanian russian spanish swedish\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vc-rWwn3xyAX"
      },
      "source": [
        "### German"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYcjc1PwPYfc"
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.snowball import GermanStemmer\n",
        "# from nltk.stem.cistem import Cistem\n",
        "from nltk.stem import LancasterStemmer\n",
        "from nltk.stem import PorterStemmer "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_VukXigQDT6"
      },
      "source": [
        "# Defining the function that will take the String/word as the input and gives us the german stemmer\n",
        "def German_Stemmer(word):\n",
        "    print (\"The string is:\",word)\n",
        "    # Snow Ball Stemmer\n",
        "    stemmer_1 = SnowballStemmer(language=\"german\")\n",
        "    print('Snowball stemmer:',stemmer_1.stem(word))\n",
        "    \n",
        "    # German Stemmer\n",
        "    stemmer_2 = GermanStemmer()\n",
        "    print('German stemmer:',stemmer_2.stem(word))\n",
        "    \n",
        "    # Cistem Stemmer\n",
        "    #stemmer_3 = Cistem()\n",
        "    #print('Cistem stemmer\\t:',stemmer_3.segment(word))\n",
        "    \n",
        "    # Lancaster Stemmer\n",
        "    stemmer_4 =LancasterStemmer()\n",
        "    print(\"Lancaster stemmer:\",stemmer_4.stem(word))\n",
        "    \n",
        "    # Porter Stemmer\n",
        "    stemmer_5 = PorterStemmer()\n",
        "    print(\"Porter stemmer:\",stemmer_5.stem(word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbFBTJtuQPt3",
        "outputId": "4ca47b7f-de5d-460b-bc99-4e0bbb4f8830"
      },
      "source": [
        "# English : Pretend\n",
        "German_Stemmer(\"Vorgeben\") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The string is: Vorgeben\n",
            "Snowball stemmer: vorgeb\n",
            "German stemmer: vorgeb\n",
            "Lancaster stemmer: vorgeb\n",
            "Porter stemmer: vorgeben\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u17eR6PtR-1r",
        "outputId": "939ee4e3-cc0d-44c8-b6d7-9b4dc3be71e1"
      },
      "source": [
        "# English : Amusement\n",
        "German_Stemmer(\"Amüsement\") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The string is: Amüsement\n",
            "Snowball stemmer: amusement\n",
            "German stemmer: amusement\n",
            "Lancaster stemmer: amüs\n",
            "Porter stemmer: amüsement\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1lmYD_JUffo",
        "outputId": "6d03151b-c644-45e8-ae02-c7be46ed3247"
      },
      "source": [
        "# English : Translate\n",
        "German_Stemmer(\"Übersetzen\") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The string is: Übersetzen\n",
            "Snowball stemmer: ubersetz\n",
            "German stemmer: ubersetz\n",
            "Lancaster stemmer: übersetz\n",
            "Porter stemmer: übersetzen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6Dk9BIeUmlW",
        "outputId": "683e6308-e484-43b5-aa2b-188f03cda934"
      },
      "source": [
        "# English : Sunrise\n",
        "German_Stemmer(\"Singen\") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The string is: Singen\n",
            "Snowball stemmer: sing\n",
            "German stemmer: sing\n",
            "Lancaster stemmer: sing\n",
            "Porter stemmer: singen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FKCGQj6AvVE"
      },
      "source": [
        "> Inference: \n",
        "\n",
        "* For Vorgeben: according to Snowball Stemmer, German stemmer,Porter Stemmer and Lancaster Stemmer,the stem we got is 'vorgeb' which means 'specified'.\n",
        "\n",
        "* For Übersetzen:  Snowball Stemmer,German stemmer,and Porter Stemmer,the stem we got is 'ubersetz' which means 'translated'. Incase of Lancaster stemmer we get 'übersetz' which also means 'translated'.\n",
        "\n",
        "* For Singen: Snowball Stemmer,German stemmer,and Lancaster Stemmer,the stem we got is 'sing' which means 'sing'. Incase of Porter stemmer we get 'singen' which also means 'singing'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmG6p141ywrw"
      },
      "source": [
        "### Spanish"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt9JgHU1y1rX"
      },
      "source": [
        "# Defining the function that will take the String/word as the input and gives us the spanish stemmer\n",
        "def Spanish_Stemmer(word):\n",
        "    print (\"The string is:\",word)\n",
        "    # Snow Ball Stemmer\n",
        "    stemmer_1 = SnowballStemmer(language='spanish')\n",
        "    print('Snowball stemmer:',stemmer_1.stem(word))\n",
        "    \n",
        "    # Porter Stemmer\n",
        "    stemmer_2 = PorterStemmer()\n",
        "    print(\"Porter stemmer:\",stemmer_2.stem(word))\n",
        "\n",
        "    # Lancaster Stemmer\n",
        "    stemmer_3 =LancasterStemmer()\n",
        "    print(\"Lancaster stemmer:\",stemmer_3.stem(word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TGBqhaX3NhH",
        "outputId": "f24ab9e2-3a20-449a-dec4-251940f176eb"
      },
      "source": [
        "# English: dancing\n",
        "Spanish_Stemmer('baile')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The string is: baile\n",
            "Snowball stemmer: bail\n",
            "Porter stemmer: bail\n",
            "Lancaster stemmer: bail\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHnB3P7f3r1M",
        "outputId": "24b28c0f-3c94-4945-9fdd-828c7a8e2aa6"
      },
      "source": [
        "# English: translator\n",
        "Spanish_Stemmer('traductora')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The string is: traductora\n",
            "Snowball stemmer: traductor\n",
            "Porter stemmer: traductora\n",
            "Lancaster stemmer: traductor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WgRlLuRAqk4"
      },
      "source": [
        "> Inference: \n",
        "\n",
        "* For baile: according to Snowball Stemmer, Porter Stemmer and Lancaster Stemmer,the stem we got is 'bail' which means 'dance'.\n",
        "\n",
        "* For traductora:  Snowball Stemmer and Lancaster Stemmer,the stem we got is 'traductor' which means 'translator'. Incase of Porter stemmer we get 'traductora' which also means 'translator'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOYB3Viy0zPR"
      },
      "source": [
        "### Portuguese"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KJEllTt06ck"
      },
      "source": [
        "from nltk.stem import RSLPStemmer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4nT83Sm07v2"
      },
      "source": [
        "# Defining the function that will take the String/word as the input and gives us the Portuguese Stemmer\n",
        "def Portuguese_Stemmer(word):\n",
        "    print (\"The string is:\",word)\n",
        "    # Snow Ball Stemmer\n",
        "    stemmer_1 = SnowballStemmer(language=\"portuguese\")\n",
        "    print('Snowball stemmer:',stemmer_1.stem(word))\n",
        "    \n",
        "    # RSLP Stemmer\n",
        "    stemmer_2 = RSLPStemmer()\n",
        "    print(\"RSLP stemmer:\",stemmer_2.stem(word))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJ2AGbc32ztL",
        "outputId": "4ee4b7cc-adee-4f91-b825-85756874e7f3"
      },
      "source": [
        "# English: playing\n",
        "Portuguese_Stemmer('jogando')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The string is: jogando\n",
            "Snowball stemmer: jog\n",
            "RSLP stemmer: jog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeEBnhlCGBce"
      },
      "source": [
        "> Inference: \n",
        "\n",
        "* According to Snowball Stemmer the stem we got is 'jog' which means 'play'.\n",
        "\n",
        "* According to RSLP Stemmer the stem we got is 'jog' which means 'play'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S41pvjaG2Lct",
        "outputId": "2f895e8a-d0f3-4ac5-b2b1-0fea518f3be6"
      },
      "source": [
        "# English: translator\n",
        "Portuguese_Stemmer('tradutora')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The string is: tradutora\n",
            "Snowball stemmer: tradutor\n",
            "RSLP stemmer: tradu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sO0KkxsAnwv"
      },
      "source": [
        "> Inference: \n",
        "\n",
        "* According to Snowball Stemmer and ArabicLight Stemmer the stem we got is 'tradutor', which means 'a translator'.\n",
        "\n",
        "* According to ISRIS Stemmer the stem we got is 'tradu' which means 'translate'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErSMkH1v6zKs"
      },
      "source": [
        "### Arabic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dy108N1R47t_"
      },
      "source": [
        "from tashaphyne.stemming import ArabicLightStemmer\n",
        "from nltk.stem.isri import ISRIStemmer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlzWgZQ14nTQ"
      },
      "source": [
        "# Defining the function that will take the String/word as the input and gives us the Arabic Stemmer\n",
        "def Arabic_Stemmer(word):\n",
        "    print (\"The string is:\",word)\n",
        "    # Snow Ball Stemmer\n",
        "    stemmer_1 = SnowballStemmer(language=\"arabic\")\n",
        "    print('Snowball stemmer:',stemmer_1.stem(word))\n",
        "    \n",
        "    # ArabicLight Stemmer\n",
        "    stemmer_2 = ArabicLightStemmer()\n",
        "    print(\"ArabicLight stemmer:\",stemmer_2.light_stem(word))\n",
        "\n",
        "    # ISRIStemmer\n",
        "    stemmer_3 = ISRIStemmer()\n",
        "    print(\"ISRIStemmer:\",stemmer_3.stem(word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vgf_87K5XuF",
        "outputId": "036d8ac8-8de7-4ea9-b6cd-0dd6ce9b2ad4"
      },
      "source": [
        "# English: translator\n",
        "Arabic_Stemmer('مترجم')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The string is: مترجم\n",
            "Snowball stemmer: مترجم\n",
            "ArabicLight stemmer: مترجم\n",
            "ISRIStemmer: ترجم\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt6e2GJPCIb1"
      },
      "source": [
        "> Inference: \n",
        "\n",
        "* According to Snowball Stemmer and ArabicLight Stemmer the stem we got is 'مترجم', it dint get changed.\n",
        "\n",
        "* According to ISRIS Stemmer the stem we got is 'ترجم' which means 'translate'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zu3fvmtdBsj-",
        "outputId": "ee19af4d-11a8-4bdc-cdc0-f5f292fc802e"
      },
      "source": [
        "# English: fairly\n",
        "Arabic_Stemmer('تماما')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The string is: تماما\n",
            "Snowball stemmer: تمام\n",
            "ArabicLight stemmer: مام\n",
            "ISRIStemmer: تما\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oj_WBuz3AkMn"
      },
      "source": [
        "> Inference: \n",
        "\n",
        "* According to Snowball Stemmer the stem we got is 'تمام' which means 'OK'.\n",
        "\n",
        "* According to ArabicLight Stemmer the stem we got is 'مام' which means 'mum' while translating it to english.\n",
        "\n",
        "* According to ISRIStemmer the stem we obtained is 'تما' which means 'Perfect'.\n"
      ]
    }
  ]
}