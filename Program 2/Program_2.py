# -*- coding: utf-8 -*-
"""2048057_Program_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WXRfOByA498S_fwnaPXaNPr6n8AzK6fj

---
# <center> **NLP PROGRAM-2** </center>
## <center> A program to count word frequency and to remove stop words </center>
#### <center> Soundarya G_ 2048057</center>
---

## Import necessary libraries
"""

import nltk
nltk.download("all")

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk import FreqDist

"""## List of stop words."""

print(set(stopwords.words('english')))

"""## Shows how stop words are removed"""

example_sent = "This is a sample sentence,showing off the stop words filtration."

stop_words = set(stopwords.words('english'))
word_tokens = word_tokenize(example_sent)

filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]
filtered_sentence = []

for w in word_tokens:
	if w not in stop_words:
		filtered_sentence.append(w)

print('Tokenized sentence:\n\t',word_tokens)
print('Removed Stop words:\n\t',filtered_sentence)

"""## Frequency of words after tokenization"""

text = "Learn and practice and learn to practice"
words = text.split()
fdist1 = FreqDist(words)
print(fdist1)
print()
fdist1.most_common()

"""## Parts of Speech for tokens"""

def parts_of_speech(txt):
  import spacy
  sp = spacy.load('en_core_web_sm')
  sentence = sp(txt)
  print("Parts of speech with tokens:")
  print("==========================")
  for word in sentence:
    print("\t",word.pos_,'\t-', word.text)

parts_of_speech(text)

"""## Main Code"""

text = '''Celebrate Independence Day every year on 15 August as a national holiday in India to commemorate the independence of the nation from the United Kingdom on 15 August 1947. Day on which the provisions of the Indian Independence Act of 1947 came into effect, which transferred legislative sovereignty to the Indian Constituent Assembly. Independence corresponded with India's partition, wherein British India had  been divided into the Dominions of India and Pakistan along religious lines.'''

def word_count(text):
  from nltk import FreqDist
  from nltk.corpus import stopwords
  from nltk.tokenize import word_tokenize

  stop_words = set(stopwords.words('english'))
  word_tokens = word_tokenize(text)

  filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]

  for w in word_tokens:
	  if w not in stop_words:
		  filtered_sentence.append(w)
    
  fdist1 = FreqDist(filtered_sentence)
  print(fdist1)
  list1 = fdist1.most_common()
  print("\n\n\t Count",'- Tokens')
  for i in list1:
     print("\t",i[1],'-', i[0])

def token():
  print('PROGRAM 2')
  print('=========')
  option = 'yes'
  while(option=='yes'):

    print('\t1. Count \n\t2. Parts of Speech')
    op = input('Select your option:')
    print('\nEnter the sentence to be tokenized:')
    text = input('\t')
    if op == '1':
      word_count(text)
    elif op == '2':
      parts_of_speech(text)
    else:
      print('Invalid input')

    option=input('\n Do you want to continue[yes/no]:\t')

token()